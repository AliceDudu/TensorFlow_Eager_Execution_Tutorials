{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hellocybernetics/TensorFlow_Eager_Execution_Tutorials/blob/master/tutorials/02_intermediate/Bidrectional_Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkJooysd9saV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "L = tf.keras.layers\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "mvWroGCq-AGM",
    "outputId": "67c5137d-eeb3-457b-a170-3033de5674ab"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data:  (60000, 28, 28)\n",
      "test_data:  (10000, 28, 28)\n",
      "training_label:  (60000,)\n",
      "test_label:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 25\n",
    "num_classes = 10\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"training_data: \", x_train.shape)\n",
    "print(\"test_data: \", x_test.shape)\n",
    "print(\"training_label: \", y_train.shape)\n",
    "print(\"test_label: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "int-eal1-RBm",
    "outputId": "483e4246-19e2-46a2-b713-e2c2979461f4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data:  (60000, 28, 28)\n",
      "test_data:  (10000, 28, 28)\n",
      "training_label:  (60000, 10)\n",
      "test_label:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train_eager = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "x_test_eager = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "y_train_eager = tf.reshape(tf.one_hot(y_train, 10), (-1, 10))\n",
    "y_test_eager = tf.reshape(tf.one_hot(y_test, 10), (-1, 10))\n",
    "\n",
    "print(\"training_data: \", x_train_eager.shape)\n",
    "print(\"test_data: \", x_test_eager.shape)\n",
    "print(\"training_label: \", y_train_eager.shape)\n",
    "print(\"test_label: \", y_test_eager.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwcKjSDNGD5W"
   },
   "source": [
    "### DataSet\n",
    "You make Dataset using `tf.data.Dataset` Class but Keras API doesn't need this dataset. If you write training loop code manually, `Dataset` class is very useful. And using keras API, you need numpy.array inputs instead of tf.Tensor. I don't know why...so you only need numpy preprocessing (or get numpy.array from tf.Tensor using numpy() method after preprocessing using function of tf).\n",
    "\n",
    "### NOTE\n",
    "This notebook we don't need 'tf.data.Dataset'. This code only just for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNU_cq4L-u10"
   },
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train_eager, y_train_eager))\n",
    "    .batch(batch_size)\n",
    "    .shuffle(10000)\n",
    ")\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_npuvGwv-588"
   },
   "outputs": [],
   "source": [
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test_eager, y_test_eager))\n",
    "    .batch(1000)\n",
    "    .shuffle(10000)\n",
    ")\n",
    "test_dataset = test_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-Z06hTWOx4P"
   },
   "source": [
    "### RNN using LSTM\n",
    "In keras API, LSTM recives inputs tensor whose shape is (batch_size, seqence_length, feature_dim), and output tensor whose shape is (batch_size, fearure_dim).When you need all time sequence data, you have to give `return_sequences=True` to LSTM's constractor. Generally, when you stack LSTM's, you need all sequence data.\n",
    "\n",
    "We use  just only `tf.keras.layers.Bidirectional` for using Bidrectional LSTM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gz5RrFnm_HzM"
   },
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, hidden_size=10, num_layers=2, num_classes=10):\n",
    "        super(RNN, self).__init__(name='mnist_rnn')\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = self.get_lstm_layers(hidden_size, num_layers)            \n",
    "        self.fc = L.Dense(num_classes, activation=\"softmax\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lstm_layers(hidden_size, num_layers):\n",
    "        lstm_layers = []\n",
    "        # we need all sequence data. write return_sequences=True! \n",
    "        for i in range(num_layers-1):\n",
    "            lstm_layers.append(\n",
    "                L.Bidirectional(\n",
    "                    L.CuDNNLSTM(units=hidden_size, \n",
    "                                         return_sequences=True)\n",
    "                )\n",
    "            )\n",
    "        # the final layer return only final sequence\n",
    "        # if you need all sequences, you have to write return_sequences=True.\n",
    "        lstm_layers.append(\n",
    "            L.Bidirectional(\n",
    "                L.CuDNNLSTM(units=hidden_size)\n",
    "            )\n",
    "        )\n",
    "        return tf.keras.Sequential(lstm_layers)\n",
    "        \n",
    "    def call(self, x):        \n",
    "        # Forward propagate LSTM\n",
    "        out = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMARKqbaBjIN"
   },
   "outputs": [],
   "source": [
    "model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "_NPkLp-rBzCp",
    "outputId": "b520e388-52fe-4bc8-e53a-7131b1788ae6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_4 (Sequential)    multiple                  5760      \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              multiple                  210       \n",
      "=================================================================\n",
      "Total params: 5,970\n",
      "Trainable params: 5,970\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Eager Execution initialize parameters when using model.call()\n",
    "model(x_train_eager[:50])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "wDEyoIMnDKEy",
    "outputId": "3f9c5d25-6472-4b7e-b3cf-28274c0d38ac"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "94/94 [==============================] - 8s 81ms/step - loss: 2.0717 - acc: 0.3138 - val_loss: 1.6623 - val_acc: 0.5032\n",
      "Epoch 2/25\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 1.3120 - acc: 0.6062 - val_loss: 0.9671 - val_acc: 0.7200\n",
      "Epoch 3/25\n",
      "94/94 [==============================] - 7s 80ms/step - loss: 0.8209 - acc: 0.7607 - val_loss: 0.6587 - val_acc: 0.8092\n",
      "Epoch 4/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.6089 - acc: 0.8215 - val_loss: 0.5239 - val_acc: 0.8412\n",
      "Epoch 5/25\n",
      "94/94 [==============================] - 7s 80ms/step - loss: 0.4958 - acc: 0.8532 - val_loss: 0.4411 - val_acc: 0.8674\n",
      "Epoch 6/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.4225 - acc: 0.8759 - val_loss: 0.3824 - val_acc: 0.8862\n",
      "Epoch 7/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.3707 - acc: 0.8913 - val_loss: 0.3413 - val_acc: 0.8983\n",
      "Epoch 8/25\n",
      "94/94 [==============================] - 7s 80ms/step - loss: 0.3359 - acc: 0.9007 - val_loss: 0.3150 - val_acc: 0.9071\n",
      "Epoch 9/25\n",
      "94/94 [==============================] - 8s 81ms/step - loss: 0.3081 - acc: 0.9083 - val_loss: 0.2964 - val_acc: 0.9110\n",
      "Epoch 10/25\n",
      "94/94 [==============================] - 7s 80ms/step - loss: 0.2853 - acc: 0.9161 - val_loss: 0.2761 - val_acc: 0.9169\n",
      "Epoch 11/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.2647 - acc: 0.9215 - val_loss: 0.2632 - val_acc: 0.9203\n",
      "Epoch 12/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.2480 - acc: 0.9272 - val_loss: 0.2520 - val_acc: 0.9262\n",
      "Epoch 13/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.2341 - acc: 0.9309 - val_loss: 0.2367 - val_acc: 0.9265\n",
      "Epoch 14/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.2200 - acc: 0.9354 - val_loss: 0.2283 - val_acc: 0.9326\n",
      "Epoch 15/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.2098 - acc: 0.9385 - val_loss: 0.2229 - val_acc: 0.9332\n",
      "Epoch 16/25\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.2012 - acc: 0.9406 - val_loss: 0.2198 - val_acc: 0.9327\n",
      "Epoch 17/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.1921 - acc: 0.9432 - val_loss: 0.2064 - val_acc: 0.9387\n",
      "Epoch 18/25\n",
      "94/94 [==============================] - 8s 84ms/step - loss: 0.1827 - acc: 0.9464 - val_loss: 0.2035 - val_acc: 0.9383\n",
      "Epoch 19/25\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.1758 - acc: 0.9493 - val_loss: 0.1989 - val_acc: 0.9406\n",
      "Epoch 20/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.1713 - acc: 0.9495 - val_loss: 0.1928 - val_acc: 0.9406\n",
      "Epoch 21/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.1653 - acc: 0.9517 - val_loss: 0.1874 - val_acc: 0.9427\n",
      "Epoch 22/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.1585 - acc: 0.9535 - val_loss: 0.1809 - val_acc: 0.9455\n",
      "Epoch 23/25\n",
      "94/94 [==============================] - 8s 80ms/step - loss: 0.1541 - acc: 0.9550 - val_loss: 0.1765 - val_acc: 0.9464\n",
      "Epoch 24/25\n",
      "94/94 [==============================] - 7s 79ms/step - loss: 0.1489 - acc: 0.9560 - val_loss: 0.1768 - val_acc: 0.9457\n",
      "Epoch 25/25\n",
      "94/94 [==============================] - 7s 80ms/step - loss: 0.1457 - acc: 0.9577 - val_loss: 0.1712 - val_acc: 0.9473\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9e57fb88d0>"
      ]
     },
     "execution_count": 50,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train_eager.numpy(), \n",
    "          y=y_train_eager.numpy(), \n",
    "          validation_split=0.2, \n",
    "          epochs=num_epochs,\n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "oDuYrgFIG850",
    "outputId": "66b7b552-046c-4811-b58e-2465dcb72676"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 9s 30ms/step\n",
      "test_accracy:  0.95\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x=x_test_eager.numpy(), \n",
    "                                     y=y_test_eager.numpy())\n",
    "\n",
    "print(\"test_accracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yeayjx_nKXaW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Recurrent_Neural_Network",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
