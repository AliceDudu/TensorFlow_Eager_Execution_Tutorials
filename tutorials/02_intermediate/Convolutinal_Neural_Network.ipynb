{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Convolutinal_Neural_Network",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hellocybernetics/TensorFlow_Eager_Execution_Tutorials/blob/master/tutorials/02_intermediate/Convolutinal_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "YUXAcV28Z5Td",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 667
        },
        "outputId": "d6bfb14d-4316-4459-f8ec-4ba4fc685076"
      },
      "cell_type": "code",
      "source": [
        "!pip install tf-nightly-2.0-preview"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tf-nightly-2.0-preview\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/f4/caced52bfdded13e1913c6c6700de847773297d53664666d54cd8f3c06d9/tf_nightly_2.0_preview-1.13.0.dev20190110-cp36-cp36m-manylinux1_x86_64.whl (74.3MB)\n",
            "\u001b[K    100% |████████████████████████████████| 74.3MB 579kB/s \n",
            "\u001b[?25hRequirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.11.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.15.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.2.1.post0)\n",
            "Collecting tb-nightly<1.14.0a0,>=1.13.0a0 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/55/4b/780b9caf8c58b3c1c664781de52f36ff482e30ca0760b1ff4f02ff757cec/tb_nightly-1.13.0a20190107-py3-none-any.whl (3.2MB)\n",
            "\u001b[K    100% |████████████████████████████████| 3.2MB 9.8MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.6)\n",
            "Requirement already satisfied: absl-py>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.6.1)\n",
            "Collecting tensorflow-estimator-2.0-preview (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e2/c4/e821cf2999928616840b24ae3b25d2713d418b713510822a0ecd829e0002/tensorflow_estimator_2.0_preview-1.13.0.dev2019011100-py2.py3-none-any.whl (243kB)\n",
            "\u001b[K    100% |████████████████████████████████| 245kB 17.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.14.6)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (1.0.5)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (0.32.3)\n",
            "Collecting google-pasta>=0.1.0 (from tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a5/e2/7d191b4613b20fa149e9ebc952f954650fc1dbcabb39e9387f6f1cd5d313/google_pasta-0.1-py3-none-any.whl (51kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 26.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tf-nightly-2.0-preview) (3.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.10 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (0.14.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a0,>=1.13.0a0->tf-nightly-2.0-preview) (3.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tf-nightly-2.0-preview) (2.8.0)\n",
            "Collecting mock>=2.0.0 (from tensorflow-estimator-2.0-preview->tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e6/35/f187bdf23be87092bd0f1200d43d23076cee4d0dec109f195173fd3ebc79/mock-2.0.0-py2.py3-none-any.whl (56kB)\n",
            "\u001b[K    100% |████████████████████████████████| 61kB 25.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tf-nightly-2.0-preview) (40.6.3)\n",
            "Collecting pbr>=0.11 (from mock>=2.0.0->tensorflow-estimator-2.0-preview->tf-nightly-2.0-preview)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f3/04/fddc1c2dd75b256eda4d360024692231a2c19a0c61ad7f4a162407c1ab58/pbr-5.1.1-py2.py3-none-any.whl (106kB)\n",
            "\u001b[K    100% |████████████████████████████████| 112kB 18.8MB/s \n",
            "\u001b[?25hInstalling collected packages: tb-nightly, pbr, mock, tensorflow-estimator-2.0-preview, google-pasta, tf-nightly-2.0-preview\n",
            "Successfully installed google-pasta-0.1 mock-2.0.0 pbr-5.1.1 tb-nightly-1.13.0a20190107 tensorflow-estimator-2.0-preview-1.13.0.dev2019011100 tf-nightly-2.0-preview-1.13.0.dev20190110\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Wzbzil3EquvD",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "\n",
        "L = tf.keras.layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "_b3wl5DyrGbv",
        "outputId": "51d82f38-a8e8-466a-9684-70343b77e387",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        }
      },
      "cell_type": "code",
      "source": [
        "# Hyper parameters\n",
        "num_epochs = 10\n",
        "num_classes = 10\n",
        "batch_size = 1024\n",
        "learning_rate = 0.001\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "\n",
        "print(\"training_data\\n\", x_train.shape)\n",
        "print(\"test_data\\n\", x_test.shape)\n",
        "print(\"training_label\\n\", y_train.shape)\n",
        "print(\"test_label\\n\", y_test.shape)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 32s 0us/step\n",
            "training_data\n",
            " (50000, 32, 32, 3)\n",
            "test_data\n",
            " (10000, 32, 32, 3)\n",
            "training_label\n",
            " (50000, 1)\n",
            "test_label\n",
            " (10000, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "zQ9dPufBDY_e",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "EIFr75qmBuM-",
        "outputId": "4c78f134-bc69-4403-a3f2-f5905e792dcf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "train_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "    .batch(batch_size)\n",
        "    .shuffle(10000)\n",
        ")\n",
        "\n",
        "train_dataset = (\n",
        "    train_dataset.map(lambda x, y: \n",
        "                      (tf.div_no_nan(tf.cast(x, tf.float32), 255.0), \n",
        "                       tf.reshape(tf.one_hot(y, 10), (-1, 10))))\n",
        ")\n",
        "\n",
        "print(train_dataset)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ((None, 32, 32, 3), (None, 10)), types: (tf.float32, tf.float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "jNdtaRRB7d5O",
        "outputId": "d1048a19-9380-4987-d2da-e260cb2b5531",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "test_dataset = (\n",
        "    tf.data.Dataset.from_tensor_slices((x_test, y_test))\n",
        "    .batch(1000)\n",
        "    .shuffle(10000)\n",
        ")\n",
        "test_dataset = (\n",
        "    test_dataset.map(lambda x, y: \n",
        "                      (tf.div_no_nan(tf.cast(x, tf.float32), 255.0), \n",
        "                       tf.reshape(tf.one_hot(y, 10), (-1, 10))))\n",
        ")\n",
        "print(test_dataset)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<MapDataset shapes: ((None, 32, 32, 3), (None, 10)), types: (tf.float32, tf.float32)>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "P5PNiSQWrfAF",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "S2mYcv4D2UGO",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Cifar10Model(tf.keras.Model):\n",
        "    def __init__(self):\n",
        "        super(Cifar10Model, self).__init__(name='cifar_cnn')\n",
        "        \n",
        "        self.conv_block1 = tf.keras.Sequential([\n",
        "            L.Conv2D(\n",
        "                8, \n",
        "                5,\n",
        "                padding='same',\n",
        "                activation=tf.nn.relu,\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(l=0.001),\n",
        "            ),\n",
        "            L.MaxPooling2D(\n",
        "                (3, 3), \n",
        "                (2, 2), \n",
        "                padding='same'\n",
        "            ),\n",
        "        ])\n",
        "\n",
        "        self.conv_block2 = tf.keras.Sequential([\n",
        "            L.Conv2D(\n",
        "                16, \n",
        "                5,\n",
        "                padding='same',\n",
        "                activation=tf.nn.relu,\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(l=0.001),\n",
        "            ),\n",
        "            L.MaxPooling2D(\n",
        "                (3, 3), \n",
        "                (2, 2), \n",
        "                padding='same',\n",
        "            ),\n",
        "        ])\n",
        "        \n",
        "        self.conv_block3 = tf.keras.Sequential([\n",
        "            L.Conv2D(\n",
        "                32, \n",
        "                5,\n",
        "                padding='same',\n",
        "                activation=tf.nn.relu,\n",
        "                kernel_regularizer=tf.keras.regularizers.l2(l=0.001),\n",
        "            ),\n",
        "            L.MaxPooling2D(\n",
        "                (3, 3), \n",
        "                (2, 2), \n",
        "                padding='same',\n",
        "            ),\n",
        "        ])\n",
        "        \n",
        "        self.flatten = L.Flatten()\n",
        "        self.fc1 = L.Dense(\n",
        "            256, \n",
        "            activation=tf.nn.relu,\n",
        "            kernel_regularizer=tf.keras.regularizers.l2(l=0.001))\n",
        "        self.dropout = L.Dropout(0.8)\n",
        "        self.fc2 = L.Dense(10)\n",
        "        self.softmax = L.Softmax()\n",
        "\n",
        "    def call(self, x, training=False):\n",
        "        x = self.conv_block1(x, training=training)\n",
        "        x = self.conv_block2(x, training=training)\n",
        "        x = self.conv_block3(x, training=training)\n",
        "        x = self.flatten(x)\n",
        "        x = self.dropout(self.fc1(x), training=training)\n",
        "        x = self.fc2(x)\n",
        "        return self.softmax(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "6VyzP7OTtDGf",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model = Cifar10Model()\n",
        "\n",
        "def loss_fn(y, y_pre):\n",
        "    return tf.keras.losses.categorical_crossentropy(y, y_pre)\n",
        "\n",
        "def accuracy(y, y_pre):\n",
        "    return tf.keras.metrics.categorical_accuracy(y, y_pre)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7zrxUvWPu6PK",
        "outputId": "e4ae9b82-6d97-44f0-ef4e-58d7c96d59c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 557
        }
      },
      "cell_type": "code",
      "source": [
        "for j in range(num_epochs):\n",
        "    \n",
        "    running_loss = 0\n",
        "    running_acc = 0\n",
        "\n",
        "    for i, (x_, y_) in enumerate(train_dataset):\n",
        "        \n",
        "        with tf.GradientTape() as tape:\n",
        "            y_pre = model(x_, training=True)\n",
        "            loss = loss_fn(y_, y_pre)\n",
        "        acc = accuracy(y_, y_pre)\n",
        "        grads = tape.gradient(loss, model.variables)\n",
        "        optimizer.apply_gradients(zip(grads, model.variables))\n",
        "        running_loss += tf.reduce_mean(loss)\n",
        "        running_acc += tf.reduce_mean(acc)\n",
        "    \n",
        "    print(\"-----epoch {} -----\".format(j + 1))\n",
        "    print(\"loss: \", running_loss.numpy()/(i + 1))\n",
        "    print(\"acc: \", running_acc.numpy()/(i + 1))    "
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "-----epoch 1 -----\n",
            "loss:  2.159227176588409\n",
            "acc:  0.18321771037821866\n",
            "-----epoch 2 -----\n",
            "loss:  1.8324200377172353\n",
            "acc:  0.3226679782478177\n",
            "-----epoch 3 -----\n",
            "loss:  1.6927250453404017\n",
            "acc:  0.3773806824976084\n",
            "-----epoch 4 -----\n",
            "loss:  1.5976887917032048\n",
            "acc:  0.41615400508958467\n",
            "-----epoch 5 -----\n",
            "loss:  1.536187775281011\n",
            "acc:  0.4423094768913425\n",
            "-----epoch 6 -----\n",
            "loss:  1.48232144725566\n",
            "acc:  0.461150344537229\n",
            "-----epoch 7 -----\n",
            "loss:  1.4467300103635203\n",
            "acc:  0.4776394513188576\n",
            "-----epoch 8 -----\n",
            "loss:  1.414776393345424\n",
            "acc:  0.4917148278684032\n",
            "-----epoch 9 -----\n",
            "loss:  1.3857308212591677\n",
            "acc:  0.5027416774204799\n",
            "-----epoch 10 -----\n",
            "loss:  1.3624782951510683\n",
            "acc:  0.5130796237867705\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "8m2XzQv3G-na",
        "outputId": "b856a057-e432-4f24-a5b1-97ec6c8c6223",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "test_accuracy = 0\n",
        "for i, (x_, y_) in enumerate(test_dataset):\n",
        "    y_pre = model(x_)\n",
        "    test_accuracy += tf.reduce_mean(accuracy(y_, y_pre))\n",
        "test_accuracy /= i + 1\n",
        "\n",
        "print(\"test accuracy {:0.3f}\".format(test_accuracy.numpy()))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test accuracy 0.556\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dnuY_yuOLrO-",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Rzz2FNtOqzdC",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}