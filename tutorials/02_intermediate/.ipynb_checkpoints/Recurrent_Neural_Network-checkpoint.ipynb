{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/hellocybernetics/TensorFlow_Eager_Execution_Tutorials/blob/master/tutorials/02_intermediate/Recurrent_Neural_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VkJooysd9saV"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "tf.enable_eager_execution()\n",
    "L = tf.keras.layers\n",
    "tfe = tf.contrib.eager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "mvWroGCq-AGM",
    "outputId": "9eed5404-af6c-4c0a-b0a7-450b13ebdf45"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data:  (60000, 28, 28)\n",
      "test_data:  (10000, 28, 28)\n",
      "training_label:  (60000,)\n",
      "test_label:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# Hyper parameters\n",
    "num_epochs = 25\n",
    "num_classes = 10\n",
    "batch_size = 512\n",
    "learning_rate = 0.001\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = tfk.datasets.mnist.load_data()\n",
    "\n",
    "print(\"training_data: \", x_train.shape)\n",
    "print(\"test_data: \", x_test.shape)\n",
    "print(\"training_label: \", y_train.shape)\n",
    "print(\"test_label: \", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 89
    },
    "colab_type": "code",
    "id": "int-eal1-RBm",
    "outputId": "79f4ab5e-1944-4ba3-f2c6-14f586b89980"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data:  (60000, 28, 28)\n",
      "test_data:  (10000, 28, 28)\n",
      "training_label:  (60000, 10)\n",
      "test_label:  (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "x_train_eager = tf.convert_to_tensor(x_train, dtype=tf.float32)\n",
    "x_test_eager = tf.convert_to_tensor(x_test, dtype=tf.float32)\n",
    "y_train_eager = tf.reshape(tf.one_hot(y_train, 10), (-1, 10))\n",
    "y_test_eager = tf.reshape(tf.one_hot(y_test, 10), (-1, 10))\n",
    "\n",
    "print(\"training_data: \", x_train_eager.shape)\n",
    "print(\"test_data: \", x_test_eager.shape)\n",
    "print(\"training_label: \", y_train_eager.shape)\n",
    "print(\"test_label: \", y_test_eager.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jwcKjSDNGD5W"
   },
   "source": [
    "### DataSet\n",
    "You make Dataset using `tf.data.Dataset` Class but Keras API doesn't need this dataset. If you write training loop code manually, `Dataset` class is very useful. And using keras API, you need numpy.array inputs instead of tf.Tensor. I don't know why...so you only need numpy preprocessing (or get numpy.array from tf.Tensor using numpy() method after preprocessing using function of tf).\n",
    "\n",
    "### NOTE\n",
    "This notebook we don't need 'tf.data.Dataset'. This code only just for reference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YNU_cq4L-u10"
   },
   "outputs": [],
   "source": [
    "train_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_train_eager, y_train_eager))\n",
    "    .batch(batch_size)\n",
    "    .shuffle(10000)\n",
    ")\n",
    "train_dataset = train_dataset.repeat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_npuvGwv-588"
   },
   "outputs": [],
   "source": [
    "test_dataset = (\n",
    "    tf.data.Dataset.from_tensor_slices((x_test_eager, y_test_eager))\n",
    "    .batch(1000)\n",
    "    .shuffle(10000)\n",
    ")\n",
    "test_dataset = test_dataset.repeat()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "e-Z06hTWOx4P"
   },
   "source": [
    "### RNN using LSTM\n",
    "In keras API, LSTM recives inputs tensor whose shape is (batch_size, seqence_length, feature_dim), and output tensor whose shape is (batch_size, fearure_dim).When you need all time sequence data, you have to give `return_sequences=True` to LSTM's constractor. Generally, when you stack LSTM's, you need all sequence data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gz5RrFnm_HzM"
   },
   "outputs": [],
   "source": [
    "class RNN(tf.keras.Model):\n",
    "    def __init__(self, hidden_size=10, num_layers=2, num_classes=10):\n",
    "        super(RNN, self).__init__(name='mnist_rnn')\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        self.lstm = self.get_lstm_layers(hidden_size, num_layers)            \n",
    "        self.fc = L.Dense(num_classes, activation=\"softmax\")\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_lstm_layers(hidden_size, num_layers):\n",
    "        lstm_layers = []\n",
    "        # we need all sequence data. write return_sequences=True! \n",
    "        for i in range(num_layers-1):\n",
    "            lstm_layers.append(\n",
    "                L.CuDNNLSTM(units=hidden_size, return_sequences=True)\n",
    "            )\n",
    "        # the final layer return only final sequence\n",
    "        # if you need all sequences, you have to write return_sequences=True.\n",
    "        lstm_layers.append(L.CuDNNLSTM(units=hidden_size))\n",
    "        return tf.keras.Sequential(lstm_layers)\n",
    "        \n",
    "    def call(self, x):        \n",
    "        # Forward propagate LSTM\n",
    "        out = self.lstm(x)\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oMARKqbaBjIN"
   },
   "outputs": [],
   "source": [
    "model = RNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 215
    },
    "colab_type": "code",
    "id": "_NPkLp-rBzCp",
    "outputId": "adbc4090-6f0b-42ab-c8f3-33cc2c96065a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential_3 (Sequential)    multiple                  2480      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              multiple                  110       \n",
      "=================================================================\n",
      "Total params: 2,590\n",
      "Trainable params: 2,590\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "model.compile(optimizer=optimizer,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=[\"accuracy\"])\n",
    "\n",
    "# Eager Execution initialize parameters when using model.call()\n",
    "model(x_train_eager[:50])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 935
    },
    "colab_type": "code",
    "id": "wDEyoIMnDKEy",
    "outputId": "8e5a9c44-cd00-4999-f450-27e45a4f4030"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "94/94 [==============================] - 4s 47ms/step - loss: 2.1856 - acc: 0.2238 - val_loss: 1.9467 - val_acc: 0.3433\n",
      "Epoch 2/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 1.6989 - acc: 0.4228 - val_loss: 1.4462 - val_acc: 0.5258\n",
      "Epoch 3/25\n",
      "94/94 [==============================] - 4s 46ms/step - loss: 1.2604 - acc: 0.6018 - val_loss: 1.0583 - val_acc: 0.6913\n",
      "Epoch 4/25\n",
      "94/94 [==============================] - 4s 46ms/step - loss: 0.9625 - acc: 0.7154 - val_loss: 0.8442 - val_acc: 0.7549\n",
      "Epoch 5/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.8116 - acc: 0.7575 - val_loss: 0.7295 - val_acc: 0.7807\n",
      "Epoch 6/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.7145 - acc: 0.7822 - val_loss: 0.6535 - val_acc: 0.8047\n",
      "Epoch 7/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.6454 - acc: 0.8027 - val_loss: 0.5999 - val_acc: 0.8229\n",
      "Epoch 8/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.5950 - acc: 0.8199 - val_loss: 0.5494 - val_acc: 0.8375\n",
      "Epoch 9/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.5501 - acc: 0.8353 - val_loss: 0.5157 - val_acc: 0.8486\n",
      "Epoch 10/25\n",
      "94/94 [==============================] - 5s 49ms/step - loss: 0.5161 - acc: 0.8470 - val_loss: 0.4818 - val_acc: 0.8593\n",
      "Epoch 11/25\n",
      "94/94 [==============================] - 4s 48ms/step - loss: 0.4854 - acc: 0.8573 - val_loss: 0.4609 - val_acc: 0.8699\n",
      "Epoch 12/25\n",
      "94/94 [==============================] - 4s 46ms/step - loss: 0.4591 - acc: 0.8662 - val_loss: 0.4296 - val_acc: 0.8768\n",
      "Epoch 13/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.4379 - acc: 0.8727 - val_loss: 0.4198 - val_acc: 0.8808\n",
      "Epoch 14/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.4195 - acc: 0.8788 - val_loss: 0.3983 - val_acc: 0.8877\n",
      "Epoch 15/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.3984 - acc: 0.8844 - val_loss: 0.3806 - val_acc: 0.8923\n",
      "Epoch 16/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.3837 - acc: 0.8886 - val_loss: 0.3670 - val_acc: 0.8953\n",
      "Epoch 17/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.3731 - acc: 0.8910 - val_loss: 0.3555 - val_acc: 0.9021\n",
      "Epoch 18/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.3582 - acc: 0.8972 - val_loss: 0.3418 - val_acc: 0.9040\n",
      "Epoch 19/25\n",
      "94/94 [==============================] - 4s 46ms/step - loss: 0.3489 - acc: 0.8992 - val_loss: 0.3324 - val_acc: 0.9060\n",
      "Epoch 20/25\n",
      "94/94 [==============================] - 4s 46ms/step - loss: 0.3364 - acc: 0.9036 - val_loss: 0.3311 - val_acc: 0.9048\n",
      "Epoch 21/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.3278 - acc: 0.9050 - val_loss: 0.3148 - val_acc: 0.9112\n",
      "Epoch 22/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.3206 - acc: 0.9071 - val_loss: 0.3051 - val_acc: 0.9127\n",
      "Epoch 23/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.3112 - acc: 0.9095 - val_loss: 0.3022 - val_acc: 0.9134\n",
      "Epoch 24/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.3028 - acc: 0.9124 - val_loss: 0.2974 - val_acc: 0.9141\n",
      "Epoch 25/25\n",
      "94/94 [==============================] - 4s 45ms/step - loss: 0.2994 - acc: 0.9126 - val_loss: 0.2902 - val_acc: 0.9162\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f9e58003470>"
      ]
     },
     "execution_count": 39,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train_eager.numpy(), \n",
    "          y=y_train_eager.numpy(), \n",
    "          validation_split=0.2, \n",
    "          epochs=num_epochs,\n",
    "          batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 53
    },
    "colab_type": "code",
    "id": "oDuYrgFIG850",
    "outputId": "afd19d09-2dd7-4575-9655-6227943ad81b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 5s 17ms/step\n",
      "test_accracy:  0.9114\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_acc = model.evaluate(x=x_test_eager.numpy(), \n",
    "                                     y=y_test_eager.numpy())\n",
    "\n",
    "print(\"test_accracy: \", test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Yeayjx_nKXaW"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "Recurrent_Neural_Network",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
